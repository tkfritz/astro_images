{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf439999",
   "metadata": {},
   "source": [
    "Neural network to get galaxy type, Here only two types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3dbc7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.cm as cm\n",
    "import sys\n",
    "import os\n",
    "#wcs is incompabible with newest numpy thus below not used \n",
    "#from astropy import wcs\n",
    "#to access astronomical images in fits format\n",
    "from astropy.io import fits\n",
    "#torch functions\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "#sklearn helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score,f1_score, log_loss\n",
    "#xgboost for comparison\n",
    "from xgboost import XGBClassifier\n",
    "#logistic regression for comparison \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef3837",
   "metadata": {},
   "source": [
    "Getting the data. This is currently only from one field, later for will be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b9700cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 43, 1, 168)\n",
      "Index(['Unnamed: 0', 'index', 'objid', 'ra', 'dec', 'psfMag_u', 'psfMag_g',\n",
      "       'psfMag_r', 'psfMag_i', 'psfMag_z', 'probPSF_u', 'probPSF_g',\n",
      "       'probPSF_r', 'probPSF_i', 'probPSF_z', 'modelMag_u', 'modelMag_g',\n",
      "       'modelMag_r', 'modelMag_i', 'modelMag_z', 'petroRad_g', 'petroRad_r',\n",
      "       'petroRad_i', 'run', 'rerun', 'camcol', 'field', 'type', 'specobjid',\n",
      "       'class', 'subclass', 'redshift', 'plate', 'mjd', 'fiberid', 'nvote',\n",
      "       'p_el', 'p_cw', 'p_acw', 'p_edge', 'p_dk', 'p_mg', 'p_el_debiased',\n",
      "       'p_cs_debiased', 'spiral', 'elliptical', 'uncertain', 'image',\n",
      "       'pixel_x', 'pixel_y', 'off_image'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cutouts1=np.load(\"stripe82_1_ell_spiral_im.npy\")\n",
    "print(cutouts1.shape)\n",
    "df1=pd.read_csv(\"stripe82_1_ell_spiral_table.csv\")\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64f1c534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#adding cpu\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9fd05437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image train data\n",
      "(100, 1, 43, 43)\n",
      "(68, 1, 43, 43)\n"
     ]
    }
   ],
   "source": [
    "target_train, target_test,image_train,image_test,df_train,df_test= train_test_split(df1.loc[:,\"spiral\"],cutouts1.T,df1,train_size=0.60, shuffle=True, random_state=1)\n",
    "print(\"shape of image train data\")\n",
    "print(image_train.shape)\n",
    "print(image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c110c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_torch(model,data):\n",
    "    y_pred_list_c = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_batch, _ in data:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_pred_list_c.append(y_test_pred.cpu().numpy())\n",
    "    y_pred_list_c = [a.squeeze().tolist() for a in y_pred_list_c]\n",
    "    return y_pred_list_c  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ce2603da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cfafe815",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train, target_test = np.array(target_train), np.array(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b6cd967",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1b64b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im_dataset = ClassificationDataset(torch.from_numpy(image_train).float(), torch.from_numpy(target_train).float())\n",
    "test_im_dataset = ClassificationDataset(torch.from_numpy(image_test).float(), torch.from_numpy(target_test).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48612e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im_loader = DataLoader(dataset=train_im_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_im_loader = DataLoader(dataset=test_im_dataset, batch_size=1)\n",
    "train_im_loader_pred = DataLoader(dataset=train_im_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd54b66",
   "metadata": {},
   "source": [
    "Test input outputs relations for network of 3 layers, it is still 3 *3 convolutional and 2 *2 maximuma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bd3e904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first conv layer input: torch.Size([1, 1, 43, 43]) output: torch.Size([1, 16, 41, 41])\n",
      "max pool input:torch.Size([1, 16, 41, 41]) output:torch.Size([1, 16, 20, 20])\n",
      "second conv layer input: torch.Size([1, 16, 20, 20]) output: torch.Size([1, 32, 18, 18])\n",
      "second max pool layer input: torch.Size([1, 32, 18, 18]) output: torch.Size([1, 32, 9, 9])\n",
      "third conv layer input: torch.Size([1, 32, 9, 9]) output: torch.Size([1, 64, 7, 7])\n",
      "third max pool layer input: torch.Size([1, 64, 7, 7]) output: torch.Size([1, 64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input0 = torch.randn(1, 1, 43, 43)\n",
    "b=torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=0)\n",
    "output0=b(input0)\n",
    "print(f\"first conv layer input: {input0.shape} output: {output0.shape}\")\n",
    "\n",
    "m = nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "#standard drops but can be changed, can also use pooling and co get better number \n",
    "output1 = m(output0)\n",
    "print(f\"max pool input:{output0.shape} output:{output1.shape}\")\n",
    "#input format (Batch, Number Channels, height, width)\n",
    "b2=torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "output2=b2(output1)\n",
    "print(f\"second conv layer input: {output1.shape} output: {output2.shape}\")\n",
    "output3 = m(output2)\n",
    "print(f\"second max pool layer input: {output2.shape} output: {output3.shape}\")\n",
    "\n",
    "b3=torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "output4=b3(output3)\n",
    "print(f\"third conv layer input: {output3.shape} output: {output4.shape}\")\n",
    "output5 = m(output4)\n",
    "print(f\"third max pool layer input: {output4.shape} output: {output5.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c6af116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBinary4(torch.nn.Module):\n",
    "    #no padding because image does not really end when the data ends. \n",
    "    def __init__(self):\n",
    "        super(CNNBinary4, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 43, 43, 1)\n",
    "        # Conv -> (?, 41, 41, 16)\n",
    "        # Pool -> (?, 20, 20, 16)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L2 ImgIn shape=(?, 20, 20, 16)\n",
    "        # Conv      ->(?, 18, 18, 32)\n",
    "        # Pool      ->(?, 9, 9, 32)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L3 ImgIn shape=(?, 9, 9, 32)\n",
    "        # Conv      ->(?, 7, 7, 64)\n",
    "        # Pool      ->(?, 3, 3, 64)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))        \n",
    "        # L3 FC 3x3x64 inputs -> 128 outputs\n",
    "        self.fc1 = torch.nn.Linear(3 * 3 * 64, 128, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L4 Final FC 128 inputs -> 1 output\n",
    "        self.fc2 = torch.nn.Linear(128, 1, bias=True) #\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight) # initialize parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out) #dont forget to add/omit layer here\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "18c57640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNBinary4(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=576, out_features=128, bias=True)\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15352/994019738.py:31: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fc1.weight)\n"
     ]
    }
   ],
   "source": [
    "keep_prob=1\n",
    "model1 =CNNBinary4()\n",
    "model1.to(device)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7f3c2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to fit it\n",
    "#parameters: model used, train_data, test_data, epchs, batch_size, learning_rate, file to collect sats, \n",
    "#optional regularization \n",
    "def torch_fit(model,train_loader,test_loader,epochs,batch_size,learning_rate,loss_stats,l2reg=0):\n",
    "    learning_rate = learning_rate\n",
    "    criterion = torch.nn.BCELoss()    # Softmax is internally computed.\n",
    "    #if no regularization\n",
    "    if l2reg==0:\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    #l2 regularization is added in optimizer as weight_decay=1e-5 or nsimilar \n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate,weight_decay=l2reg)        \n",
    "    print(\"Begin training.\")\n",
    "    for e in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "        # TRAINING\n",
    "        train_epoch_loss = 0\n",
    "        model.train()\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            y_train_pred = model(X_train_batch)\n",
    "        \n",
    "            train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "        # VALIDATION    \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            test_epoch_loss = 0\n",
    "        \n",
    "            model.eval()\n",
    "            for X_test_batch, y_test_batch in test_loader:\n",
    "                X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "            \n",
    "                y_test_pred = model(X_test_batch)\n",
    "                        \n",
    "                test_loss = criterion(y_test_pred, y_test_batch.unsqueeze(1))\n",
    "            \n",
    "                test_epoch_loss += test_loss.item()\n",
    "        loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "        loss_stats['test'].append(test_epoch_loss/len(test_loader))                              \n",
    "    \n",
    "        print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Test Loss: {test_epoch_loss/len(test_loader):.5f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1d09738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acb32f0c8d9412b8315fbc7e53ba1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 002: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 003: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 004: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 005: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 006: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 007: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 008: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 009: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 010: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 011: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 012: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 013: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 014: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 015: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 016: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 017: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 018: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 019: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 020: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 021: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 022: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 023: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 024: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 025: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 026: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 027: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 028: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 029: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 030: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 031: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 032: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 033: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 034: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 035: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 036: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 037: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 038: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 039: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 040: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 041: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 042: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 043: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 044: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 045: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 046: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 047: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 048: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 049: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 050: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 051: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 052: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 053: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 054: | Train Loss: 49.21875 | Test Loss: 32.35294\n",
      "Epoch 055: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 056: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 057: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 058: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 059: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 060: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 061: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 062: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 063: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 064: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 065: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 066: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 067: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 068: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 069: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 070: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 071: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 072: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 073: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 074: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 075: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 076: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 077: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 078: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 079: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 080: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 081: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 082: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 083: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 084: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 085: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 086: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 087: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 088: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 089: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 090: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 091: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 092: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 093: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 094: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 095: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 096: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 097: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 098: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 099: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 100: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 101: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 102: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 103: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 104: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 105: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 106: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 107: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 108: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 109: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 110: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 111: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 112: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 113: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 114: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 115: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 116: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 117: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 118: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 119: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 120: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 121: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 122: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 123: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 124: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 125: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 126: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 127: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 128: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 129: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 130: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 131: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 132: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 133: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 134: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 135: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 136: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 137: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 138: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 139: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 140: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 141: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 142: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 143: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 144: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 145: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 146: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 147: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 148: | Train Loss: 27.34375 | Test Loss: 32.35294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 150: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 151: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 152: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 153: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 154: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 155: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 156: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 157: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 158: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 159: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 160: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 161: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 162: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 163: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 164: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 165: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 166: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 167: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 168: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 169: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 170: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 171: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 172: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 173: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 174: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 175: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 176: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 177: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 178: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 179: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 180: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 181: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 182: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 183: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 184: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 185: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 186: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 187: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 188: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 189: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 190: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 191: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 192: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 193: | Train Loss: 38.28125 | Test Loss: 32.35294\n",
      "Epoch 194: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 195: | Train Loss: 43.75000 | Test Loss: 32.35294\n",
      "Epoch 196: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 197: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 198: | Train Loss: 32.81250 | Test Loss: 32.35294\n",
      "Epoch 199: | Train Loss: 27.34375 | Test Loss: 32.35294\n",
      "Epoch 200: | Train Loss: 38.28125 | Test Loss: 32.35294\n"
     ]
    }
   ],
   "source": [
    "#somehow nothing improves like sometimes, unclear what is reason mistake in setup or real chance ? \n",
    "loss_stats_test = {\n",
    "    'train': [], 'test': []\n",
    "}\n",
    "torch_fit(model1,train_im_loader,test_im_loader,200,32,0.01,loss_stats_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ab1f2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68,) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(target_test.shape,target_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f9753a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_test=pred_torch(model1,test_im_loader)\n",
    "c_train=pred_torch(model1,train_im_loader_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a89bec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f8bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
